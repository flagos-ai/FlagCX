flagcx (0.7-1) unstable; urgency=medium

  * Added support to TsingMicro, including device adaptor tsmicroAdaptor and CCL adaptor tcclAdaptor.
  * Implemented an experimental kernel-free non-reduce collective communication (SendRecv, AlltoAll, AlltoAllv, Broadcast, Gather, Scatter, AllGather) using device-buffer IPC/RDMA.
  * Enabled auto-tuning on NVIDIA, MetaX, and Hygon platforms, achieving 1.02×–1.26× speedups for AllReduce, AllGather, ReduceScatter, and AlltoAll.
  * Enhanced flagcxNetAdaptor with one-sided primitives (put, putSignal, waitValue) and added retransmission support for reliability improvement.

 -- FlagOS Contributors <contact@flagos.io>  Sat, 01 Nov 2025 10:00:00 +0800

flagcx (0.6-1) unstable; urgency=medium

  * Implemented device-buffer IPC communication to support intra-node SendRecv operations.
  * Introduced device-initiated, host-launched device-side primitives, enabling kernel-based communication directly from devices.
  * Enhanced auto-tuning with 50% performance improvement on MetaX platforms for the AllReduce operations.

 -- FlagOS Contributors <contact@flagos.io>  Wed, 01 Oct 2025 10:00:00 +0800

flagcx (0.5-1) unstable; urgency=medium

  * Added support for AMD GPUs, including a device adaptor hipAdaptor and a CCL adaptor rcclAdaptor.
  * Introduced flagcxNetAdaptor to unify network backends, currently supporting socket, IBRC, UCX and IBUC (experimental).
  * Enabled zero-copy device-buffer RDMA (user-buffer RDMA) to boost performance for small messages.
  * Supported auto-tuning in homogeneous scenarios via flagcxTuner.
  * Added test automation in CI/CD for PyTorch APIs.

 -- FlagOS Contributors <contact@flagos.io>  Mon, 01 Sep 2025 10:00:00 +0800

flagcx (0.4-1) unstable; urgency=medium

  * Supported heterogeneous training of ERNIE4.5 (Baidu) on NVIDIA and Iluvatar GPUs with Paddle + FlagCX.
  * Improved heterogeneous communication across arbitrary NIC configurations, with more robust and flexible deployments.
  * Introduced an experimental network plugin interface with extended supports for IBRC and SOCKET. Device buffer registration now can be done via DMA-BUF.
  * Added an InterOp-level DSL to enable customized C2C algorithm design.
  * Provided user documentation under docs/.

 -- FlagOS Contributors <contact@flagos.io>  Fri, 01 Aug 2025 10:00:00 +0800

flagcx (0.3-1) unstable; urgency=medium

  * Integrated three additional native communication libraries: HCCL (Huawei), MUSACCL (Moore Threads) and MPI.
  * Enhanced heterogeneous collective communication operations with pipeline optimizations.
  * Introduced device-side functions to enable device-buffer RDMA, complementing the existing host-side functions.
  * Delivered a full-stack open-source solution, FlagScale + FlagCX, for efficient heterogeneous prefilling-decoding disaggregation.

 -- FlagOS Contributors <contact@flagos.io>  Tue, 01 Jul 2025 10:00:00 +0800

flagcx (0.2-1) unstable; urgency=medium

  * Integrated 3 additional native communications libraries, including MCCL (Moore Threads), XCCL (Mellanox) and DUCCL (BAAI).
  * Improved 11 heterogeneous collective communication operations with automatic topology detection and full support to single-NIC and multi-NIC environments.

 -- FlagOS Contributors <contact@flagos.io>  Thu, 01 May 2025 10:00:00 +0800

flagcx (0.1-1) unstable; urgency=medium

  * Added 5 native communications libraries including CCL adaptors for NCCL (NVIDIA), IXCCL (Iluvatar), and CNCL (Cambricon), and Host CCL adaptors GLOO and Bootstrap.
  * Supported 11 heterogeneous collective communication operations using the C2C (Cluster-to-Cluster) algorithm.
  * Provided a full-stack open-source solution, FlagScale + FlagCX, for efficient heterogeneous training.
  * Natively integrated into PaddlePaddle [v3.0.0](https://github.com/PaddlePaddle/Paddle/tree/v3.0.0), with support for both dynamic and static graphs.

 -- FlagOS Contributors <contact@flagos.io>  Tue, 01 Apr 2025 10:00:00 +0800
